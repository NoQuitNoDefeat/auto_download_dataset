import os
import requests
import shutil
from tqdm import tqdm

# ================= 1. æ•°æ®é›†ä¸‹è½½é“¾æ¥é…ç½® =================

DOWNLOAD_DIR = "datasets_downloaded"

DATASET_URLS = {
    # --- RATM (TII Racing) ---
    # è‡ªåŠ¨
    "ratm_autonomous": [
        "https://github.com/tii-racing/drone-racing-dataset/releases/download/v3.0.0/autonomous_zipchunk01",
        "https://github.com/tii-racing/drone-racing-dataset/releases/download/v3.0.0/autonomous_zipchunk02",
        "https://github.com/tii-racing/drone-racing-dataset/releases/download/v3.0.0/autonomous_zipchunk03"
    ],
    # äººå·¥
    "ratm_piloted": [
        "https://github.com/tii-racing/drone-racing-dataset/releases/download/v3.0.0/piloted_zipchunk01",
        "https://github.com/tii-racing/drone-racing-dataset/releases/download/v3.0.0/piloted_zipchunk02",
        "https://github.com/tii-racing/drone-racing-dataset/releases/download/v3.0.0/piloted_zipchunk03",
        "https://github.com/tii-racing/drone-racing-dataset/releases/download/v3.0.0/piloted_zipchunk04",
        "https://github.com/tii-racing/drone-racing-dataset/releases/download/v3.0.0/piloted_zipchunk05",
        "https://github.com/tii-racing/drone-racing-dataset/releases/download/v3.0.0/piloted_zipchunk06",
        "https://github.com/tii-racing/drone-racing-dataset/releases/download/v3.0.0/piloted_zipchunk07"
    ],

    # --- UZH FPV (Indoor Forward Facing) ---
    # åŒ…å«å›¾åƒã€IMU å’Œ Ground Truth
    "uzh_indoor_forward": [
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_forward_3_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_forward_5_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_forward_6_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_forward_7_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_forward_9_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_forward_10_snapdragon_with_gt.zip"
    ],

    # --- UZH FPV (Indoor 45 Degree Facing) ---
    "uzh_indoor_45": [
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_45_2_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_45_4_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_45_9_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_45_12_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_45_13_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/indoor_45_14_snapdragon_with_gt.zip"
    ],

    # --- UZH FPV (Outdoor Forward Facing) ---
    "uzh_outdoor_forward": [
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/outdoor_forward_1_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/outdoor_forward_3_snapdragon_with_gt.zip",
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/outdoor_forward_5_snapdragon_with_gt.zip"
    ],

    # --- UZH FPV (Outdoor 45 Degree Facing) ---
    "uzh_outdoor_45": [
        "http://rpg.ifi.uzh.ch/datasets/uzh-fpv/outdoor_45_1_snapdragon_with_gt.zip"
    ]
    #
    # # --- GRASP (UPenn)å¤±æ•ˆ ---
    # "grasp": [
    #     "http://mrsl.grasp.upenn.edu/ke/dataset/fla_wg_10.bag"
    # ],
    #
    # # --- Blackbird (MIT)ä¹Ÿå¤±æ•ˆ ---
    # # æ³¨æ„: ä¸‹è½½çš„æ˜¯ .torrent ç§å­æ–‡ä»¶
    # "blackbird": [
    #     "https://academictorrents.com/download/eb542a231dbeb2125e4ec88ddd18841a867c2656.torrent"
    # ]
}


# ================= 2. é«˜é€Ÿä¸‹è½½æ¨¡å— =================

def download_file_fast(url, folder_path):
    """
    ä½¿ç”¨ requests å’Œ tqdm å®ç°å¸¦æœ‰è¿›åº¦æ¡çš„æµå¼ä¸‹è½½
    """
    # ä» URL æå–æ–‡ä»¶å
    local_filename = url.split('/')[-1].split('?')[0]
    local_path = os.path.join(folder_path, local_filename)

    # ç®€å•çš„æ–­ç‚¹ç»­ä¼ æ£€æµ‹ï¼šå¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°åŒ¹é…ï¼ˆè¿™é‡Œæš‚ä¸æ ¡éªŒHashï¼‰ï¼Œåˆ™è·³è¿‡
    try:
        response = requests.get(url, stream=True)
        total_size = int(response.headers.get('content-length', 0))
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨: {url}\né”™è¯¯: {e}")
        return None

    if os.path.exists(local_path):
        existing_size = os.path.getsize(local_path)
        if existing_size == total_size and total_size > 0:
            print(f"  [è·³è¿‡] æ–‡ä»¶å·²å­˜åœ¨ä¸”å®Œæ•´: {local_filename}")
            return local_path
        elif existing_size > 0:
            print(f"  [è¦†ç›–] æ–‡ä»¶ä¸å®Œæ•´ï¼Œé‡æ–°ä¸‹è½½: {local_filename}")

    # å¼€å§‹ä¸‹è½½
    block_size = 1024 * 1024  # 1MB ç¼“å†²åŒº
    progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True, desc=local_filename, ncols=100)

    try:
        with open(local_path, 'wb') as file:
            for data in response.iter_content(block_size):
                progress_bar.update(len(data))
                file.write(data)
        progress_bar.close()
        return local_path
    except Exception as e:
        progress_bar.close()
        print(f"âŒ ä¸‹è½½ä¸­æ–­: {e}")
        return None


# ================= 3. åˆå¹¶ä¸æ¸…ç†æ¨¡å— =================

def merge_ratm_files(dataset_name, folder_path, file_list):
    """
    ä¸“é—¨å¤„ç† RATM çš„åˆ†å·åˆå¹¶é€»è¾‘
    """
    # ç¡®å®šåˆå¹¶åçš„ç›®æ ‡æ–‡ä»¶å
    if "autonomous" in dataset_name:
        target_name = "autonomous.zip"
    elif "piloted" in dataset_name:
        target_name = "piloted.zip"
    else:
        return  # ä¸æ˜¯ RATM æ•°æ®é›†ï¼Œæ— éœ€åˆå¹¶

    target_path = os.path.join(folder_path, target_name)

    if os.path.exists(target_path):
        print(f"  âœ… åˆå¹¶ç›®æ ‡å·²å­˜åœ¨ ({target_name})ï¼Œè·³è¿‡åˆå¹¶ã€‚")
        return

    print(f"  âš™ï¸ æ­£åœ¨åˆå¹¶ {len(file_list)} ä¸ªåˆ†å·æ–‡ä»¶... (è¯·å‹¿å…³é—­)")

    try:
        # æŒ‰é¡ºåºåˆå¹¶æ–‡ä»¶
        # è¿™é‡Œçš„ file_list é¡ºåºå¾ˆé‡è¦ï¼Œä»£ç é€»è¾‘ä¾èµ–äº DATASET_URLS é‡Œçš„åˆ—è¡¨é¡ºåº
        with open(target_path, 'wb') as outfile:
            for chunk_path in file_list:
                if not chunk_path or not os.path.exists(chunk_path):
                    print(f"  âŒ é”™è¯¯: ç¼ºå°‘åˆ†å· {chunk_path}ï¼Œæ— æ³•åˆå¹¶ã€‚")
                    return

                # æµå¼å¤åˆ¶ï¼Œé¿å…å†…å­˜æº¢å‡º
                with open(chunk_path, 'rb') as infile:
                    shutil.copyfileobj(infile, outfile)

        print(f"  âœ… åˆå¹¶æˆåŠŸ: {target_name}")

        # åˆ é™¤åŸå§‹åˆ†å·
        print(f"  ğŸ§¹ æ­£åœ¨åˆ é™¤åŸå§‹åˆ†å·ä»¥é‡Šæ”¾ç©ºé—´...")
        for chunk_path in file_list:
            os.remove(chunk_path)
        print("  âœ… æ¸…ç†å®Œæˆã€‚")

    except Exception as e:
        print(f"  âŒ åˆå¹¶å¤±è´¥: {e}")


# ================= 4. ä¸»ç¨‹åºå…¥å£ =================

def main():
    if not os.path.exists(DOWNLOAD_DIR):
        os.makedirs(DOWNLOAD_DIR)

    print(f"{'=' * 40}")
    print("   è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸‹è½½ç®¡ç†å™¨ (Pythonå®Œæ•´ç‰ˆ)")
    print(f"{'=' * 40}")
    print(f"ä¸‹è½½ç›®å½•: {os.path.abspath(DOWNLOAD_DIR)}\n")

    for name, urls in DATASET_URLS.items():
        print(f"ğŸš€ æ­£åœ¨å¤„ç†æ•°æ®é›†: [{name}]")

        dataset_path = os.path.join(DOWNLOAD_DIR, name)
        if not os.path.exists(dataset_path):
            os.makedirs(dataset_path)

        downloaded_files = []
        all_success = True

        # 1. ä¸‹è½½é˜¶æ®µ
        for url in urls:
            file_path = download_file_fast(url, dataset_path)
            if file_path:
                downloaded_files.append(file_path)
            else:
                all_success = False

        # 2. åˆå¹¶é˜¶æ®µ (ä»…å½“ä¸‹è½½å…¨éƒ¨æˆåŠŸä¸”æ˜¯ RATM æ—¶)
        if all_success and "ratm" in name:
            merge_ratm_files(name, dataset_path, downloaded_files)

        print("-" * 40)

    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼")
    # print("æ³¨æ„: Blackbird ä¸‹è½½çš„æ˜¯ç§å­æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨è¿…é›·/BTè½¯ä»¶æ‰“å¼€ä¸‹è½½å®é™…æ•°æ®ã€‚")


if __name__ == "__main__":
    main()